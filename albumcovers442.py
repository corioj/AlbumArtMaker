# -*- coding: utf-8 -*-
"""AlbumCovers442.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1arzheO0zzxaLCBkY2wtdDSi0m9xgqrOe

# Starting

Importing necessary modules
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import itertools
import random
from matplotlib import image
import glob as glob
import pickle
import time
import pandas as pd
from PIL import Image

#torch
!pip install torchsummary
import torch
import torchvision
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader, Dataset
from torchsummary import summary
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.nn.functional as F
from torchvision.utils import make_grid

from torchvision.datasets import ImageFolder, MNIST, FashionMNIST

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    print("Using the GPU!")
else:
    print("WARNING: Could not find GPU! Using CPU only.")

"""# Downloading Dataset and Building Dataloaders

First, the dataset scraped from Spotify's album cover collection is downloaded.
"""

# Commented out IPython magic to ensure Python compatibility.
#from google.colab import drive
#drive.mount('/content/drive', force_remount=True)
# %cd "/content/zip_files"
#!ls

# DOWNLOAD DATASET HERE
#!unzip -q mini-edges2shoes.zip
# SPLIT UP YOUR FILES PLEASE, 26,000 AT A TIME
!unzip -q AA1.zip
print("aa1")
!unzip -q AA2.zip
print("aa2")
#!unzip -q AA3.zip
#print("aa3")
#!unzip -q AA4.zip
#print("aa4")
#!unzip -q AA5.zip
#print("aa5")
#!unzip -q AA6.zip
#print("aa6")
#!unzip -q AA7.zip
#print("aa7")

# Commented out IPython magic to ensure Python compatibility.
# genre dictionary
# %cd "/content/zip_files"
genre_dict = {
    "slowcore" : 0,
    "trap" : 1,
    "dance pop" : 2,
    "alternative metal" : 3,
    "shoegaze" : 4,
    "k pop" : 5,
    "indietronica" : 6,
    "indie rock" : 7,
    "emo rap" : 8,
    "house" : 9,
    "pop punk" : 10,
    "singer songwriter" : 11,
    "progressive house" : 12,
    "j pop" : 13,
    "punk" : 14,
    "classical" : 15,
    "glitchcore" : 16,
    "grunge" : 17,
    "trance" : 18,
    "neo-psychedelic" : 19,
    "noise pop" : 20,
    "post punk" : 21,
    "minimal techno" : 22,
    "post rock" : 23,
    "experimental rock" : 24,
    "vaporwave" : 25,
    "glitch" : 26,
    "ambient pop" : 27,
    "drone" : 28,
    "nightcore" : 29
}

# for row in each csv, delete /Users/johncorio/Documents/EECS 442 Files/project/AlbumArt/ from 'Art Link' col
# use dictionary to replace genre name
# data list
train_data = []
# iterate over csvs
# FIRST TWO ZIP FILES GO FROM ALBUM 0 UP TO ALBUM 50000
# AS SUCH ONLY NEED FIRST TWO CSVS IN THIS LIST
# UNCOMMENT WHEN YOU WANT ALL DATA FILES
for file in ['ProjectData.csv', 'ProjectData1First.csv']:#,'ProjectData2.csv', 'ProjectData4.csv', 'ProjectData1.csv', 'ProjectData3.csv']:
    # read data and process it
    data = pd.read_csv(file)
    genres = np.array([genre_dict[val] for val in data["Genre"]])
    images = [st.replace('/Users/johncorio/Documents/EECS 442 Files/project/AlbumArt/', '') for st in data['Art Link']]
    # iterate and append each training example
    for i in range(len(genres)):
        if images[i] == 'Album50001.jpg':
            break
        if np.asarray(Image.open(images[i])).shape == (64, 64, 3):
            train_data.append([images[i], genres[i]])
    print(file + " done reading")

print(len(train_data)) # should be 157115 or something like that, less with shape cleaning
for i in range(1,10):
    print(train_data[i])
    print(train_data[-1*i])

"""Next, we parse the dataset and build our dataloaders to be used in training """

#img = train_data[70]
#print(img)
#!ls
#path = "/content/drive/MyDrive/442Project/AlbumArt/"
#print(path + img[0])
#im = Image.open(path + img[0])

# in case we mess up run time
# !rm -rf <zip_files

# check our zip downloads properly
# print(len(glob.glob('*'))) # should be > 157116

class AlbumCovers(Dataset):
    def __init__(self, train_data = None, transform=None):

        self.transform = transform
        self.files = train_data 

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        img = Image.open(self.files[idx][0])
        label = self.files[idx][1]
        img = np.asarray(img)
        if self.transform:
            img = self.transform(img)
        return img, label

#standard transform
transform = transforms.Compose([
transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])

batch_size = 128
num_classes = 10

#%cd "AlbumArt"
#!ls

##BUILD DATALOADER USING data_loader = torch.utils.data.DataLoader()
dataset = AlbumCovers(train_data = train_data, transform = transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


def imshow(inp, title=None, ax=None, figsize=(5, 5)):
  """Imshow for Tensor."""
  inp = inp.numpy().transpose((1, 2, 0))
  if ax is None:
    fig, ax = plt.subplots(1, figsize=figsize)
  ax.imshow(inp)
  ax.set_xticks([])
  ax.set_yticks([])
  if title is not None:
    ax.set_title(title)

# Visualize training partition
# Get a batch of training data
# , classes
inputs, classes = next(iter(dataloader))

# Make a grid from batch
out = torchvision.utils.make_grid(inputs, nrow=8)

fig, ax = plt.subplots(1, figsize=(64, 64))
print(classes)
imshow(out, ax=ax) # title=' | '.join(title),

transform = transforms.Compose([
transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])

batch_size = 32
num_classes = 10

dataloader = torch.utils.data.DataLoader(FashionMNIST('data', train=True, download=True, transform=transform),
                                          batch_size=batch_size, shuffle=True)

"""# Building the Generator and Discriminator

First, we build the Generator with the following architecture:
"""

def normal_init(m, mean, std):
    """
    Helper function. Initialize model parameter with given mean and std.
    """
    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):
        m.weight.data.normal_(mean, std)
        m.bias.data.zero_()

class Generator(nn.Module):
    # initializers
    def __init__(self):
        super(Generator, self).__init__()
        
        #self.embed = nn.Embedding(num_classes, num_classes)
        self.embed = nn.Embedding(num_classes, latent_dim)

        self.lin = nn.Linear(110, 1024)
        self.model = nn.Sequential(
            nn.ConvTranspose2d(100, 512, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
            
            #nn.Linear(110, 256),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Linear(256, 512),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Linear(512, 1024),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Linear(1024, 2048),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Linear(2048, 4096),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Linear(4096, 8192),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Linear(8192, 12288),
            #nn.Tanh()
        )

    # weight_init
    def weight_init(self, mean, std):
        for m in self._modules:
            normal_init(self._modules[m], mean, std)

    # forward method
    def forward(self, input, labels):
        label = self.embed(labels)
        label = label.view(label.size(0), 100, 1, 1)
        input = torch.mul(label, input.view(input.size(0), 100, 1, 1))
        #input = torch.cat((input.view(input.size(0), 100), self.embed(label)), -1)
        #input = self.lin(input)
        #input = input.view(-1, 1024, 1, 1)
        img = self.model(input)
        return img

"""Next, we build the Discriminator with the following architecture:"""

class Discriminator(nn.Module):
    # initializers
    def __init__(self):
        super(Discriminator, self).__init__()
        
        self.embed = nn.Embedding(num_classes, 3*64*64)

        self.model = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),
            nn.Sigmoid()
               
            #nn.Linear(12298, 8192),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Dropout(0.3),
            #nn.Linear(8192, 4096),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Dropout(0.3),
            #nn.Linear(4096, 2048),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Dropout(0.3),
            #nn.Linear(2048, 1024),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Dropout(0.3),
            #nn.Linear(1024, 512),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Dropout(0.3),
            #nn.Linear(512, 256),
            #nn.LeakyReLU(0.2, inplace=True),
            #nn.Dropout(0.3),
            #nn.Linear(256, 1),
            #nn.Sigmoid()
        )
               
    # weight_init
    def weight_init(self, mean, std):
        for m in self._modules:
            normal_init(self._modules[m], mean, std)

    # forward method
    def forward(self, image, labels):
        label = self.embed(labels)
        label = label.view(label.size(0), 3, 64, 64)
        input = torch.cat((image, label), 1)
        d_out = self.model(input)
        return d_out

"""# Training

We train the Generator and Discriminator for 20 epochs.
"""

generator = Generator()
discriminator = Discriminator()
generator.cuda()
discriminator.cuda()
criterion = nn.BCELoss()
lr = 0.001
latent_dim = 100
d_optimizer = torch.optim.Adam(discriminator.parameters(), lr)
g_optimizer = torch.optim.Adam(generator.parameters(), lr)
generator.weight_init(mean=0.0, std=0.02)
discriminator.weight_init(mean=0.0, std=0.02)
generator.train()
discriminator.train()
torch.autograd.set_detect_anomaly(True)

def train(generator, discriminator, num_epochs = 20):
    for epoch in range(num_epochs):
        print('Start training epoch %d' % (epoch + 1))
        D_losses = []
        G_losses = []
        num_iter = 0
        for i, (images, labels) in enumerate(dataloader):
            images = images.type(torch.FloatTensor).cuda()
            labels = labels.type(torch.LongTensor).cuda()
            #--------------------------------
            #z = torch.randn(batch_size, latent_dim, 1, 1).cuda()
            z = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).cuda()
            fake_labels = torch.LongTensor(np.random.randint(0, num_classes, batch_size)).cuda()

            # Train Generator
            g_optimizer.zero_grad()

            # Generate a batch of images
            gen_images = generator(z, fake_labels).cuda()

            # generator loss
            result_D = discriminator(gen_images, fake_labels).cuda()
            loss_G = criterion(result_D, (torch.ones(result_D.size())).cuda())

            loss_G.backward()
            g_optimizer.step()
       
            #--------------------------------
            #z = torch.randn(batch_size, latent_dim, 1, 1).cuda()
            z = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).cuda()
            fake_labels = torch.LongTensor(np.random.randint(0, num_classes, batch_size)).cuda()

            # Train Discriminator
            d_optimizer.zero_grad()

            # real images
            d_real = discriminator(images, labels).cuda()
            d_real_loss = criterion(d_real, (torch.ones(d_real.size())).cuda())

            # fake images
            gen_images = generator(z, fake_labels).cuda()
            d_fake = discriminator(gen_images, fake_labels).cuda()
            d_fake_loss = criterion(d_fake, torch.zeros(d_fake.size()).cuda()).cuda()

            # discriminator loss
            loss_D = d_real_loss + d_fake_loss
            loss_D.backward()
            d_optimizer.step()

            #--------------------------------
            D_losses.append(loss_D)
            G_losses.append(loss_G)
            num_iter += 1
  
        print('loss of discriminator D: %.3f' % (torch.mean(torch.FloatTensor(D_losses))))
        print('loss of generator G: %.3f' % (torch.mean(torch.FloatTensor(G_losses))))

train(generator, discriminator, num_epochs = 20)
# D loss goes way down very easily
# G loss spikes

"""#Generate Images using Generator

Choose a label and see what the trained generator produces
"""

z = torch.randn(100, 100).cuda()
labels =torch.LongTensor([i for _ in range(10) for i in range(10)]).cuda()
sample_images = generator(z, labels).data.cpu()
grid = make_grid(sample_images, nrow=10, normalize=True).permute(1,2,0).numpy()
fig, ax = plt.subplots(figsize=(15,15))
ax.imshow(grid)
_ = plt.yticks([])
_ = plt.xticks(np.arange(15, 700, 70), ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], rotation=45, fontsize=20)

#prob should build a "show image" function to make this a one-liner, which takes noise and a label into the generator and produces an image
def print_images(Generator = generator, imgs, labels):
    labels = torch.LongTensor(np.arange(10)).cuda()
    predicted_images = Generator(imgs, labels)
    fig, axes = plt.subplots(2, 2)
    axes = np.reshape(axes, (4, ))
    for i in range(4):
        if i == 1 || i == 3:
            axes[i].imshow(predicted_imgs[i])
        else:
            axes[i].imshow(imgs[i])
        axes[i].axis('off')
    
    plt.tight_layout()
    label_epoch = 'Epoch {0}'.format(num_epoch)
    fig.text(0.5, 0, , ha='center')
    label_input = 'Input'
    fig.text(0.25, 1, label_input, ha='center')
    label_output = 'Output'
    fig.text(0.75, 1, label_output, ha='center')

    plt.show()